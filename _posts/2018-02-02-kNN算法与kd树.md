---
layout: post
title:  $k$NN算法与$kd$树
date:   2018-02-02
category: 机器学习
tag: 
---

* content
{:toc}

#### 优缼点

* 优点：没有显示的训练过程，理论简单，实现方便；可用于非线性分类；对数据没有假设；

* 缺点：对噪声点比较敏感；内存开销大；计算开销大；

#### 三要素

* 距离度量

设特征空间$\chi$是$n$维实数向量空间$R^n$，$x_i,x_j\in \chi,\quad x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T, \quad x_j=(x_j^{(1)},x_j^{(2)},\cdots,x_j^{(n)})^T$, $x_i,x_j$的$L_p$距离定义为：

$$L_p(x_i,x_j)=\left(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p\right) ^{\frac{1}{p}}$$

* $k$值的选择

$k$值的选择会对$k$近邻法的结果产生重大影响。  

如果选择较小的$k$值，就相当于较小的领域中的训练实例进行预测，缺点是预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，$k$值的减小就意味着整体模型变得复杂，容易过拟合。  

如果选择较大的$k$值，在较大的邻域中进行预测，可以减少学习的估计误差，缺点是较远的点也会对预测起作用，使预测发生错误。这时整体的模型变得简单。  

如果$k=N$，那么无论输入实例是什么，都将简地预测它属于训练集中实例最多的类。这时模型过于简单，完全忽略训练实例中的大量有用信息。在应用中一般使用交叉验证法选择合适的$k$值。

* 分类决策规则

一般采用多数表决，属于经验风险最小化模型。

#### $kd$树

$kd$树的本质是多维空间中的平衡二叉树。







