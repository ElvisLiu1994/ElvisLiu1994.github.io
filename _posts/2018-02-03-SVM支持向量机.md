---
layout: post
title: SVM支持向量机
categories: 机器学习
description: 
keywords: SVM
---

* content
{:toc}

#### 简介

支持向量机是一种二类分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机。支持向量机包括核技巧，这使它成为实质上的非线性分类器。支持向量机的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的最优化算法。

支持向量机一共可以分为三类，**线性可分支持向量机、线性支持向量机、非线性支持向量机**。当数据线性可分时，通过硬间隔最大化，学习一个线性的分类器，即线性可分支持向量机；当训练数据接近线性可分时，通过软间隔最大化，也学习一个线性的分类器，即线性支持向量机；当训练数据不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。

#### 函数间隔和几何间隔

一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度。在超平面$w \cdot x_i+b = 0$确定的情况下，$\|w \cdot x_i+b\|$能够相对地表示点$x$距离超平面的远近。而$w \cdot x_i+b$的符号与类标签$y_i$的符号是否一致能够表示分类是否正确。所以可用量$\hat{\gamma} = y_i(w \cdot x_i + b)$来表示分类的正确性及确信度，这就是函数间隔的概念。

我们也可以从另一个方面理解函数间隔为什么可以用来表示确信度。在logistic回归中，我们使用$h_\theta\(x\) = \frac{1}{1+e^{-\theta^T x}}$来表示分类为正类的概率，这里的$\theta^T x$即与上面的$w \cdot x+b$相同，当它的值越大时，表示分类为正类的概率越大，所以从这个方面来解释，也可以说明函数间隔可以用来表示分类正确性及确信度。

几何间隔需要用函数间隔除以$\|\|w\|\|$，即$\frac{y_i(w \cdot x_i + b)}{\|\|w\|\|}$，或者$\gamma = y_i\left(\frac{w}{\|\|w\|\|} \cdot x_i + \frac{b}{\|\|w\|\|}\right)$。从几何角度来看，几何间隔为点到直线的垂直距离。我们也可以从向量的角度理解这两个距离，对于直线而言，向量(w,b)为直线$wx+b=0$的法向量，点$\vec x$与直线的距离为其到法向量的投影与法向量模长的乘积，当法向量为单位向量时，该值表示几何距离。

#### 线性可分支持向量机--最大几何间隔法

假设数据是线性可分的，SVM的思想则是寻找一个划分超平面，使得所有训练数据到该平面的最小几何间隔最大。即：

$$
\begin{align}
\max_{w,b} \quad &\gamma \\
s.t. \quad &y_i\left(\frac{w}{||w||} \cdot x_i + \frac{b}{||w||}\right) \geq \gamma, \quad i = 1,2,\cdots,N
\end{align}
$$

考虑到几何间隔与函数间隔之间的关系，我们可以将上式改写为：

$$
\begin{align}
\max_{w,b} \quad &\frac{\hat{\gamma}}{||w||} \\
s.t. \quad &y_i\left(w \cdot x_i + b\right) \geq \hat{\gamma}, \quad i = 1,2,\cdots,N
\end{align}
$$

我们知道，实际上函数间隔$\hat{\gamma}$的取值并不会影响最优化问题的解。比如对上式中约束部分左右两边同时除以$\hat{\gamma}$，那么约束不等式变为$y_i\left(\frac{w}{\hat{\gamma}} \cdot x_i + \frac{b}{\hat{\gamma}}\right) \geq 1$，然后我们使用新的$w_0, b_0$来替代上式中的线性表达式，即令$w_0 = \frac{w}{\hat{\gamma}}$。则有$y_i\left(w_0 \cdot x_i + b_0\right) \geq 1$，而在目标函数中也用$w_0$进行替代则有$max \quad \frac{\hat{\gamma}}{\|\|w_0\|\| \hat{\gamma}}$ 则目标式可以化简为最大化$\frac{1}{\|\|w_0\|\|}$，再统一用$w$替代$w_0$，同时注意到最大化$\frac{1}{\|\|w\|\|}$和最小化$\frac{1}{2}\|\|w^2\|\|$是等价的，这里主要是为了统一最优化问题的形式，而除2是为了方便求导计算，于是就得到了标准的**线性可分支持向量机**学习的最优化问题

$$
\begin{align}
\min_{w,b} \quad &\frac{1}{2}||w||^2 \\
s.t. \quad &y_i\left(w \cdot x_i + b\right) - 1 \geq 0, \quad i = 1,2,\cdots,N
\end{align}
$$

这是一个凸二次规划问题，我们可以直接用求解约束最优化问题的方法来进行求解参数，而不使用梯度下降法这样的迭代求解方法。

###### 学习的对偶算法












